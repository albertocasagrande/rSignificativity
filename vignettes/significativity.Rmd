---
title: "Agreement Value Significativity"
output: rmarkdown::html_vignette
description: >
  Measuring the significativity of agreement values
vignette: >
  %\VignetteIndexEntry{Agreement Value Significativity}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  markdown:
    wrap: 72
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Agreement measures, such as Cohen's $\kappa$ [@Cohen:1960] and
$\textrm{IA}$ [@Casagrande:2020b],
gauge the agreement between two discrete
classifiers mapping their confusion matrices into real values.

```{r}
# create a confusion matrix
M <- matrix(1:9, nrow = 3, ncol = 3)
print(M)

# rSignificativity implements some agreement measures
rSignificativity::cohen_kappa(M)
rSignificativity::scott_pi(M)
rSignificativity::IA(M)
```

When a golden standard classifier is available,
the best among a set of classifiers can be selected
by picking the one whose confusion matrix with respect to
the golden standard has the maximal agreement
value.

The agreement values alone lack of a
meaningfulness indication. Understanding whether
two classifiers *significantly* agree on the considered
dataset is not possible by simply looking at the
corresponding agreement value.

Let $\mathcal{M}_{n,m}$ be the set of
$n \times n$-confusion matrices built over a dataset
having size $m$, i.e., the elements of any matrix
$\mathcal{M}_{n,m}$ sum up to $m$.
For any agreement measure $\sigma$, the
$\sigma$-significativity of $c$ in $\mathcal{M}_{n,m}$ is
$$
\varrho_{\sigma, n, m}(c) \stackrel{\tiny\textrm{def}}{=} \frac{\left|\left\{M \in \mathcal{M}_{n,m} \, |\, \sigma(M) < c\right\}\right|}{\left|\mathcal{M}_{n,m}\right|}.
$$
The $\sigma$-significativity is the [0,1]-normalised number of matrices
in $\mathcal{M}_{n,m}$ having a $\sigma$-value greater than $c$.
From a different point of view, it corresponds to
the probability of choosing by chance a matrix whose $\sigma$-value greater
than $c$.

Since $|\mathcal{M}_{n,m}|=\binom{n^2+m-1}{m}$, computing $\varrho_{\sigma, n, m}(c)$
takes time $O\left(n^2\binom{n^2+m-1}{m}\right)$. Nevertheless,
the Monte Carlo method [@montecarlo] can estimate $\varrho_{\sigma, n, m}(c)$ 
with $N$ samples in time $\Theta(N(n^2+m))$ with an error proportional to $1/\sqrt{N}$ 
(e.g., see [@Mackay1998]).

The function `significativity()` lets us to both exactly compute $\varrho_{\sigma, n, m}(c)$
and estimate it by using the Monte Carlo method.

```{r}
library("rSignificativity")

# evaluate kappa-significativity of 0.5 in M_{2,100} with 10000 samples
significativity(cohen_kappa, c = 0.5, n = 2, m = 100)

# evaluate kappa-significativity of 0.5 in M_{2,100} with 1000 samples
significativity(cohen_kappa, c = 0.5, n = 2, m = 100, number_of_samples = 1000)

# evaluate kappa-significativity of 0.5 in M_{2,5} with 1000 samples
significativity(cohen_kappa, c = 0.5, n = 2, m = 5, number_of_samples = 1000)

# exactly compute kappa-significativity of 0.5 in M_{2,5}
significativity(cohen_kappa, c = 0.5, n = 2, m = 5, number_of_samples = NULL)
```

The $\sigma$-significativity of $c$ in $\mathcal{P}_{n}$,
where $\mathcal{P}_{n}$ is the set of all the $n \times n$-probability
matrices, is
$$
\rho_{\sigma, n}(c) \stackrel{\tiny\textrm{def}}{=} \frac{V\left(\left\{M \in \mathcal{P}_{n} \, |\, \sigma(M) < c\right\}\right)}{V(\Delta^{(n^2-1)})}
$$
where $\Delta^{(k-1)} =\left\{ \langle x_1, \ldots x_k \rangle \in \mathbb{R}_{\geq 0}^k\, |\, \sum_{i=1}^k x_i = 1\right\}$ 
is the $k$-dimensional probability simplex and $V(\cdot)$ is the $n^2-1$-dimensional Lebesgue measure [@significativity].

If $\sigma(M) < c$ is definable in an o-minimal theory [@Dries_1998], such as in the case
of Cohen's $\kappa$ and $\textrm{IA}*$, then
$$
\lim_{m \rightarrow +\infty}\varrho_{\sigma, n, m}(c) = \rho_{\sigma, n}(c).
$$


We can estimate $\rho_{\sigma, n}(c)$ by
using the Monte Carlo method with $N$ samples in time $\Theta(n^2 N)$.
The function `significativity()` also implements this algorithm.

```{r}
# evaluate kappa-significativity of 0.5 in P_{2} with 1000 samples
significativity(cohen_kappa, 0.5, n = 2, number_of_samples = 1000)

# evaluate kappa-significativity of 0.5 in P_{2} with 10000 samples
significativity(cohen_kappa, 0.5, n = 2)

# successive calls to Monte Carlo methods may produce different results
significativity(cohen_kappa, 0.5, n = 2)

# setting the random seed before the call guarantee repeatability
set.seed(1)
significativity(cohen_kappa, 0.5, n = 2)

set.seed(1)
significativity(cohen_kappa, 0.5, n = 2)
```